# 1. 生产者分区机制和策略

# 1.1 分区原因

* 方便在集群中扩展，每个Partition可以通过调整以适应它所在的机器，1个topic又可以有多个 Partition组成，因此整个集群就可以适应任意大小的数据了。

* 提高并发，因为以Partition为单位读写


## 1.2 分区器

客户端可以控制将消息发送到哪个分区。

Producer发送的数据会封装成1个 ProducerRecord对象。

发送消息的几种重载方法:
```
ProducerRecord(@NotNull String topic, Integer partition, Long timestamp, String key, String value, @Nullable Iterable<Header> headers)
ProducerRecord(@NotNull String topic, Integer partition, Long timestamp, String key, String value)
ProducerRecord(@NotNull String topic, Integer partition, String key, String value, @Nullable Iterable<Header> headers)
ProducerRecord(@NotNull String topic, String key, String value)
ProducerRecord(@NotNull String topic, String value)
```

* 如果记录中指定了分区，则直接使用
* 如果未指定 partition，但指定了key值，则根据key的hash值 与partition数 取余计算，得出一个分区（相同的key所发送到的Partition是同一个，可用来保证消息的局部有序性）
* 如果未指定 partition，也未指定key值，则以 '黏性分区' 策略（2.4版本以前使用轮询策略）选择一个分区(【round-robin算法】第1次调用时随机生成1个整数，后面每次调用在这个整数上递增，将这个值与 topic可用的partition总数取余得到partition值)


## 1.3 分区策略

### 轮询策略
**org.apache.kafka.clients.producer.RoundRobinPartitioner**

如果key值为null，并且使用了默认的分区器，Kafka会根据轮循（Random Robin）策略将消息均匀地分布到各个分区上。


### 散列策略
**Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions**

如果key不为null，并且使用了默认的分区器，Kafka会对键进行散列，然后根据散列值把消息映射到对应的分区上。


### 黏性分区策略
**org.apache.kafka.clients.producer.UniformStickyPartitioner**

key为null，而且kafka版本 > 2.4  并且使用默认的分区器，使用黏性分区策略。

很多时候消息是没有指定Key的。而Kafka 2.4 之前的策略是轮询策略，这种策略在使用中性能比较低。所以2.4中版本加入了黏性分区策略（Sticky Partitioning Strategy）。

黏性分区器（Sticky Partitioner）主要思路是选择单个分区发送所有无Key的消息。一旦这个分区的batch已满或处于“已完成”状态，黏性分区器会随机地选择另一个分区并会尽可能地坚持使用该分区——象黏住这个分区一样


### 自定义策略
默认分区器是使用次数最多的分区器。

除了散列分区之外，用户可以根据需要对数据使用不一样的分区策略实现org.apache.kafka.clients.producer.Partitioner接口，在配置中设置实现的类prop.put("partitioner.class", 实现类);
