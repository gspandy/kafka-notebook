# 1. kafka 安装

kafka 依赖于 zookeeper，kafka启动前，要先启动 zookeeper。


## 1.1 下载并解压

* kafka-{version}.src.tar.gz: 源码包
* kafka-{version}.tar.gz: 安装包(一定要下载这个)

`注意`: 安装源码包，最后启动的时候会报错

1.下载kafka安装包:

```shell script
cd /opt
wget http://archive.apache.org/dist/kafka/2.6.0/kafka_2.12-2.6.0.tgz
```


2.解压

将安装包上传到服务器 /opt 目录下，解压:

```shell script
tar -zxcf kafka_2.12-2.6.0.tgz
```


## 1.2 修改 kafka 配置文件 server.properties

1.查看 kafka 下 config 下所有文件

```
cd /opt/kafka_2.12-2.6.0/config/

ll

# 输出
total 72
-rw-r--r-- 1 root root  906 Jul 29 02:16 connect-console-sink.properties
-rw-r--r-- 1 root root  909 Jul 29 02:16 connect-console-source.properties
-rw-r--r-- 1 root root 5321 Jul 29 02:16 connect-distributed.properties
-rw-r--r-- 1 root root  883 Jul 29 02:16 connect-file-sink.properties
-rw-r--r-- 1 root root  881 Jul 29 02:16 connect-file-source.properties
-rw-r--r-- 1 root root 2247 Jul 29 02:16 connect-log4j.properties
-rw-r--r-- 1 root root 2540 Jul 29 02:16 connect-mirror-maker.properties
-rw-r--r-- 1 root root 2262 Jul 29 02:16 connect-standalone.properties
-rw-r--r-- 1 root root 1221 Jul 29 02:16 consumer.properties
-rw-r--r-- 1 root root 4674 Jul 29 02:16 log4j.properties
-rw-r--r-- 1 root root 1925 Jul 29 02:16 producer.properties
-rw-r--r-- 1 root root 6827 Nov  5 08:32 server.properties
-rw-r--r-- 1 root root 1032 Jul 29 02:16 tools-log4j.properties
-rw-r--r-- 1 root root 1169 Jul 29 02:16 trogdor.conf
-rw-r--r-- 1 root root 1205 Jul 29 02:16 zookeeper.properties
```


2.修改 server.properties 文件

**Server Basics**

* broker_id = unique number 

broker_id: 也就是一台服务器，必须是唯一的整数

```
############################# Server Basics #############################

# The id of the broker. This must be set to a unique integer for each broker.
broker.id=1
```

* delete.topic.enable 设置为 true

`注意`: kafka 2.6 中server.properties 文件无该参数。

如果 server.properties 文件有该参数的话，设置为 true。
```
# Switch to enable topic deletion or not, default value is false
delete.topic.enable=true
```

**Log Basics**

* log.dirs 设置 kafka 运行数据和日志目录

创建 logs 文件夹:
```
cd /opt/kafka
mkdir logs
```

本次设置为: /opt/kafka/logs

```
############################# Log Basics #############################

# A comma separated list of directories under which to store log files
# 之前的暂存数据目录
log.dirs=/tmp/kafka-logs
# 更改后的暂存数据目录
log.dirs=/opt/kafka/logs
```


**Log Retention Policy**

* log.retention.hours 设置 暂存数据(log file)的有效时间

log.retention.hours 默认为 7天

```
# The minimum age of a log file to be eligible for deletion due to age
log.retention.hours=168
```

* log.retention.bytes: 暂存文件最大大小

log.retention.bytes暂存文件最大大小，默认为 1G。

```
# A size-based retention policy for logs. Segments are pruned from the log unless the remaining
# segments drop below log.retention.bytes. Functions independently of log.retention.hours.
#log.retention.bytes=1073741824
```


**Zookeeper**

* zookeeper.connect: 配置连接 zookeeper 集群地址

```
# Zookeeper connection string (see zookeeper docs for details).
# This is a comma separated host:port pairs, each corresponding to a zk
# server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".
# You can also append an optional chroot string to the urls to specify the
# root directory for all kafka znodes.
zookeeper.connect=localhost:2181
```


## 1.3 设置 kafka 环境变量

1.编辑 /etc/profile

```
[root@www ~]# sudo vi /etc/profile
```

2.配置 KAFKA环境
```
# KAFKA
export KAFKA_HOME=/opt/kafka
export PATH=$PATH:$KAFKA_HOME/bin
```

3.激活KAFKA环境
```
[root@www ~]# source /etc/profile
```

## 1.4 开启、关闭、重启 kafka服务

`注意`: 开启kafka服务前，一定要开启zookeeper服务。

1.后台启动 kafka服务

`说明`: 因为kafka启动后是阻塞进程，需要加入 -daemon 设置为后台启动
```
cd cd /opt/kafka_2.12-2.6.0/
bin/kafka-server-start.sh -daemon config/server.properties
```

2.关闭 kafka服务

```
cd cd /opt/kafka_2.12-2.6.0/
bin/kafka-server-stop.sh -daemon config/server.properties
```

* kafka-console-consumer.sh: 测试环境消费者数据
* kafka-console-producer.sh: 测试环境生产者者数据
* kafka-topics.sh: topic的操作使用此文件
* kafka-producer-perf-test.sh: 生产者的测试
* kafka-consumer-perf-test.sh: 消费者的测试